# 로컬 LLM (Llama 3, Mistral 등) 활용을 위한 사전 준비 가이드

로컬 환경에서 LLM을 구동하기 위해서는 하드웨어 사양, 소프트웨어 환경, 그리고 적절한 모델 선택이 필수적입니다. 아래에 단계별 준비 사항을 정리했습니다.

## 1. 하드웨어 요구사항 (Hardware)

로컬 LLM 구동의 핵심은 **VRAM(비디오 메모리)**입니다. 모델의 파라미터 수(크기)와 양자화(Quantization) 수준에 따라 필요한 사양이 달라집니다.

### GPU (그래픽 카드) - 가장 중요
*   **권장**: NVIDIA GeForce RTX 3060 (12GB) 이상.
*   **최소**: NVIDIA GeForce RTX 3050 (8GB) 또는 4GB 이상의 VRAM을 가진 GPU (소형 모델 구동 가능).
*   **고사양**: RTX 3090/4090 (24GB) - 70B 크기의 중대형 모델도 양자화하여 구동 가능.
*   **참고**: Mac M1/M2/M3 칩셋은 통합 메모리 덕분에 가성비 좋게 고용량 모델 구동이 가능합니다.

### 시스템 메모리 (RAM)
*   일반적으로 **16GB 이상** 권장.
*   GPU VRAM이 부족할 경우 시스템 RAM으로 일부를 오프로드(Offload)할 수 있으나 속도가 현저히 느려집니다. (특히 GGUF 포맷 사용 시 중요)
*   32GB 이상이면 쾌적합니다.

### CPU
*   최신 멀티코어 프로세서 (AVX2 명령어 지원 필수).
*   GPU가 없거나 VRAM이 부족하여 CPU로만 구동할 경우, 코어 수가 많고 메모리 대역폭이 높을수록 유리합니다.

### 저장공간 (Storage)
*   **NVMe SSD** 필수. 모델 로딩 속도에 큰 영향을 미칩니다.
*   모델 파일 하나가 작게는 4GB에서 크게는 수십 GB에 달하므로 여유 공간(100GB 이상) 확보 필요.

---

## 2. 소프트웨어 환경 (Software)

### 필수 드라이버
*   **NVIDIA GPU 사용 시**: 최신 [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) 및 그래픽 드라이버 설치.
*   **AMD GPU 사용 시**: ROCm 지원 확인 및 설치.

### 기본 도구
*   **Python**: 대부분의 AI 툴이 Python 기반이므로 설치 필요 (보통 3.10 또는 3.11 버전 권장).
*   **Git**: 각종 저장소 및 코드를 다운로드하기 위해 필요.

---

## 3. 구동을 위한 도구 (Inference Engines & UI)

초보자부터 전문가까지 수준에 맞는 도구를 선택하세요.

| 도구명 | 특징 | 추천 대상 |
| :--- | :--- | :--- |
| **LM Studio** | 설치 후 모델 검색/다운로드/채팅이 원스톱으로 가능. 설정이 매우 간편함. | **입문자 (강추)** |
| **Ollama** | 터미널 기반이지만 설치가 쉽고 가벼움. WebUI 등 다른 툴과 연동성이 좋음. | **입문자/개발자** |
| **Text-Generation-WebUI** | 'Oobabooga'로 불림. 가장 많은 기능과 설정을 지원하지만 설치 및 사용이 다소 복잡함. | **숙련자** |
| **KoboldCPP** | GGUF 모델 구동에 특화됨. 가볍고 단일 파일로 실행 가능. | **중급자** |

---

## 4. 모델 포맷 이해하기 (Model Formats)

다운로드할 때 파일 형식을 잘 봐야 합니다.

*   **GGUF (.gguf)**: **가장 추천**. CPU와 GPU를 혼합하여 사용할 수 있어 범용성이 뛰어납니다. (Ollama, LM Studio, KoboldCPP 등에서 사용)
*   **AWQ / GPTQ**: GPU 전용 양자화 포맷. 속도가 빠르지만 VRAM 용량에 엄격합니다.
*   **EXL2**: ExLlamaV2 기반. 현재 가장 빠른 추론 속도를 보여주지만 설정이 까다로울 수 있습니다.
*   **SafeTensors**: 일반적으로 변환되지 않은 원본 모델(fp16/bf16). 용량이 매우 큼.

## 5. 모델 다운로드 사이트
*   **Hugging Face (허깅페이스)**: [https://huggingface.co/](https://huggingface.co/)
    *   검색 팁: 모델명 뒤에 `GGUF`를 붙여 검색하면 사용하기 편한 버전을 찾을 수 있습니다. (예: `Llama-3-8B-Instruct-GGUF`)
